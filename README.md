### Hi there, I'm Philipp üëã

I'm an AI Scientist specializing in Generative AI and Natural Language Processing. My passion is building intelligent systems that can decipher complex, human-generated data ‚Äì from ancient languages to modern clinical records.

Below is a selection of key projects I've led or significantly contributed to during my time at the Klaus-Tschira-Institute (Dieterich Lab).

---

### üöÄ Key Projects & Contributions

#### 1. Architect of `biolm_utils`: A Deep Learning Framework for Bioinformatics
*   **Project:** [`dieterich-lab/biolm_utils`](https://github.com/dieterich-lab/biolm_utils)
*   **My Role:** **Lead Architect & Developer.** This framework is the heart of our bioinformatics research. I designed and built it from the ground up to be a modular and extensible engine for deep learning on biological sequences. It handles the entire ML lifecycle, including data processing, model training, prediction, and Explainable AI (XAI) integration.

#### 2. Application: `rna_protein_xlnet` (Transformer-based Modeling)
*   **Project:** [`dieterich-lab/rna_protein_xlnet`](https://github.com/dieterich-lab/rna_protein_xlnet)
*   **My Role:** **Lead Developer.** This project is a key application built on top of the `biolm_utils` framework. I demonstrated the framework's power by integrating a state-of-the-art Transformer model (XLNet) as a plugin to accurately predict RNA and protein half-lives from sequence data.

#### 3. Application: `rna_saluki_cnn` (CNN-based Modeling)
*   **Project:** [`dieterich-lab/rna_saluki_cnn`](https://github.com/dieterich-lab/rna_saluki_cnn)
*   **My Role:** **Core Contributor.** In this project, we showcased the framework's flexibility by implementing a custom, lightweight CNN architecture (Saluki) as another model plugin, applying it to the same sequence-to-outcome task.

#### 4. LLM Relations: Knowledge Graph Generation from Scientific Text
*   **Project:** `dieterich-lab/LLM_Relations`
*   **My Role:** **Lead Developer & Architect.** Separate from the bioinformatics framework, I designed and implemented this system for using Large Language Models (Mistral, Llama) to extract molecular interaction data and construct a biological relation graph.
*   *Note: My specific contribution can be viewed in the project's subdirectories.*

---

### üåç Community & Open Source Contributions

#### Ancient Egyptian Translation Dataset
*   **Hugging Face Link:** [`phiwi/bbaw_egyptian`](https://huggingface.co/datasets/phiwi/bbaw_egyptian)
*   **Context:** This project connects my unique background in both Egyptology and Computational Linguistics. I curated, processed, and uploaded this dataset to the Hugging Face Hub to make it accessible for the community, enabling new research in low-resource machine translation. This work is directly related to my IWSLT 2019 publication on translating ancient texts with Transformer models.

---

### üõ†Ô∏è My Tech Stack

*   **Languages:** Python, Bash, Cypher, Java
*   **AI/ML:** PyTorch, Huggingface, Langchain, LlamaIndex, Scikit-Learn, Spacy
*   **Infrastructure & Data:** Docker, Git, HPC/Slurm, Neo4j, FAISS, MongoDB

---

[<img src="https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/linkedin.svg" alt="'linkedin'" width="40"/>](https://www.linkedin.com/in/philipp-wiesenb/)   [<img src="https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/googlescholar.svg" alt="google scholar" width="40"/>](https://scholar.google.de/citations?user=zdNEDqgAAAAJ&hl=de)¬†   [<img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.svg" alt="hugging face" width="40"/>](https://huggingface.co/phiwi) 
